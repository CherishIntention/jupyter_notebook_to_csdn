{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3160752",
   "metadata": {},
   "source": [
    "### 丢弃法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb1aef",
   "metadata": {},
   "source": [
    "#### 除了权重衰减以外，深度学习常常使用丢弃法（dropout）来应对过拟合问题。丢弃法有一些不同的变体。本文中提到的丢弃法特指倒置丢弃法（inverted dropout）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015517a",
   "metadata": {},
   "source": [
    "#### 当对隐藏层使用丢弃法时，隐藏单元将有一定的概率被丢弃。设丢弃概率为p，丢弃概率是丢弃法的超参数。具体来说，设随机变量，εi为0或1的概率分别为p和1-p。使用丢弃法时，重新计算隐藏单元，hi'=εi*hi/(1-p)。由于E(εi)=1-p，因此E(hi')=E(εi)*hi/(1-p)=hi。由此可知，丢弃法并不改变输入值的期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3b8ff",
   "metadata": {},
   "source": [
    "#### 由于在训练中，隐藏层的神经元是随机丢弃的，即每个神经元的输出都可能被清零，输出层的计算无法过度依赖任何一个神经元，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，为了得到更加确定性的结果，一般不使用丢弃法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5ca31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon, nd, autograd, init\n",
    "from mxnet.gluon import loss as gloss, data as gdata, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8664836",
   "metadata": {},
   "source": [
    "##### 测试nd.random.uniform函数的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0f548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nd.random.uniform(0, 1, shape=(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7249c0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.5488135  0.5928446  0.71518934 0.84426576 0.60276335]\n",
       " [0.8579456  0.5448832  0.8472517  0.4236548  0.6235637 ]\n",
       " [0.6458941  0.3843817  0.4375872  0.2975346  0.891773  ]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b892fcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 1. 0. 0. 0.]\n",
       " [0. 1. 0. 1. 0.]\n",
       " [0. 1. 1. 1. 0.]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = w<0.6\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0df32c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.5488135 0.5928446 0.        0.        0.       ]\n",
       " [0.        0.5448832 0.        0.4236548 0.       ]\n",
       " [0.        0.3843817 0.4375872 0.2975346 0.       ]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask*w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f8f28",
   "metadata": {},
   "source": [
    "### 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "412ab32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    \"\"\"以drop_prob的概率随机丢弃NDArray类型的X中的元素\"\"\"\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob = 1-drop_prob\n",
    "    # 这种情况下，把全部元素都丢弃\n",
    "    if keep_prob==0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob\n",
    "    return mask*X / keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6892a",
   "metadata": {},
   "source": [
    "#### 运行几个例子，测试dropout函数。其中丢弃概率分别为0, 0.5, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95912b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape(2,8)\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1f544d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  4.  0.  0.  0. 12.  0.]\n",
       " [ 0.  0. 20.  0.  0. 26.  0.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b078e4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95532e",
   "metadata": {},
   "source": [
    "#### 定义模型参数。使用Fashion_MNIST数据集，定义两层隐藏层的多层感知机，其中两个隐藏层的单元个数都是256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5cb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs , num_hiddens1))\n",
    "b1 = nd.zeros(shape=(num_hiddens1))\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1 , num_hiddens2))\n",
    "b2 = nd.zeros(shape=(num_hiddens2))\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2 , num_outputs))\n",
    "b3 = nd.zeros(shape=(num_outputs))\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e8d3d",
   "metadata": {},
   "source": [
    "#### 定义模型。通常的建议是靠近输入层的丢弃概率设置的小一点。在这个实验中，把第一个隐藏层的丢弃概率设置为0.2， 第二个隐藏层的丢弃概率设置为0.5。通过is_training函数判断运行模式为训练还是测试，并只需要在训练模式下使用丢弃法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4163cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0b8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1)+b1).relu()\n",
    "    if autograd.is_training():\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    H2 = (nd.dot(H1, W2)+b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return nd.dot(H2, W3)+b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a2db3",
   "metadata": {},
   "source": [
    "#### 训练和测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e23615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1165, train acc 0.562, test acc 0.785\n",
      "epoch 2, loss 0.5763, train acc 0.784, test acc 0.810\n",
      "epoch 3, loss 0.4883, train acc 0.822, test acc 0.854\n",
      "epoch 4, loss 0.4424, train acc 0.839, test acc 0.858\n",
      "epoch 5, loss 0.4166, train acc 0.849, test acc 0.864\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 5, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377a607",
   "metadata": {},
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6c2b0",
   "metadata": {},
   "source": [
    "#### 在Gluon中，只需要在全连接层后添加Dropout层并指定丢弃概率。在训练模型时，Dropout层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时，Dropout层并不发挥作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39615610",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256,activation='relu'),\n",
    "       nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层\n",
    "       nn.Dense(256, activation='relu'),\n",
    "       nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层\n",
    "       nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49717c0",
   "metadata": {},
   "source": [
    "#### 训练并测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632a7426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1389, train acc 0.558, test acc 0.783\n",
      "epoch 2, loss 0.5777, train acc 0.786, test acc 0.831\n",
      "epoch 3, loss 0.4957, train acc 0.819, test acc 0.847\n",
      "epoch 4, loss 0.4498, train acc 0.836, test acc 0.857\n",
      "epoch 5, loss 0.4213, train acc 0.846, test acc 0.863\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669009f",
   "metadata": {},
   "source": [
    "### forward propagation, backward propagation and computational graph given in this link.\n",
    "http://zh.gluon.ai/chapter_deep-learning-basics/backprop.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_gluon",
   "language": "python",
   "name": "py36_gluon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
